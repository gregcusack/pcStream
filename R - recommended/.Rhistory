EERs = c()
for (testUser in 1:7)
{
pred = SubjectResult[[10]][[7]][[testUser]][[1]][[2]]
pred=prediction(pred[[1]],pred[[2]])
p = performance(pred,"fpr","fnr")
d = abs(slot(p,"x.values")[[1]]-slot(p,"y.values")[[1]])
EERs[testUser] = slot(p,"x.values")[[1]][which.min(d)]
plot(performance(pred,"fpr"))
par(new=TRUE)
plot(performance(pred,"fnr"))
}
mean(EERs)
EERs
#C:\R_Expr\Collection3_Results\6e200090ee
#load each Collection3_result Rdata at a time
EERs = c()
for (testUser in 1:7)
{
pred = SubjectResult[[20]][[7]][[testUser]][[1]][[2]]
pred=prediction(pred[[1]],pred[[2]])
p = performance(pred,"fpr","fnr")
d = abs(slot(p,"x.values")[[1]]-slot(p,"y.values")[[1]])
EERs[testUser] = slot(p,"x.values")[[1]][which.min(d)]
plot(performance(pred,"fpr"))
par(new=TRUE)
plot(performance(pred,"fnr"))
}
mean(EERs)
EERs
#C:\R_Expr\Collection3_Results\6e200090ee
#load each Collection3_result Rdata at a time
EERs = c()
for (testUser in 1:7)
{
pred = SubjectResult[[50]][[7]][[testUser]][[1]][[2]]
pred=prediction(pred[[1]],pred[[2]])
p = performance(pred,"fpr","fnr")
d = abs(slot(p,"x.values")[[1]]-slot(p,"y.values")[[1]])
EERs[testUser] = slot(p,"x.values")[[1]][which.min(d)]
plot(performance(pred,"fpr"))
par(new=TRUE)
plot(performance(pred,"fnr"))
}
mean(EERs)
EERs
#C:\R_Expr\Collection3_Results\6e200090ee
install.packages("streamMOA")
library("streamMOA", lib.loc="~/R/win-library/3.2")
library("streamMOA", lib.loc="~/R/win-library/3.2")
m=matrix(3,nrow=10,ncol=4)
mean(m)
install.packages("sendmailR")
library("sendmailR", lib.loc="~/R/win-library/3.2")
mime_part(iris)
from <- sprintf("<sendmailR@\\%s>", Sys.info()[4])
to <- "<ymirsky1@gmail.com>"
subject <- "Hello from R"
body <- list("It works!", mime_part(iris))
sendmail(from, to, subject, body,
control=list(smtpServer="ASPMX.L.GOOGLE.COM"))
install.packages("mailR")
library("mailR", lib.loc="~/R/win-library/3.2")
sender <- "ymirsky1@gmail.com" # Replace with a valid address
recipients <- c("ymirsky1@gmail.com") # Replace with one or more valid addresses
email <- send.mail(from = sender,
to = recipients,
subject="Subject of the email",
body = "Body of the email",
smtp = list(host.name = "aspmx.l.google.com", port = 25),
authenticate = TRUE,
send = FALSE)
detach("package:mailR", unload=TRUE)
library("mailR", lib.loc="~/R/win-library/3.2")
install.packages("mail")
detach("package:mailR", unload=TRUE)
library("mail", lib.loc="~/R/win-library/3.2")
sendmail("ymirsky1@gmail.com", "R notice", "Calculation finished.\nFetch your data!")
dim(expand.grid(seq(5,600,10),seq(.1,2,.15)))
dim(expand.grid(seq(5,300,5),seq(2,50)))
library("stream", lib.loc="~/R/win-library/3.2")
stream <- DSD_BarsAndGaussians(noise=.05)
# we set Cm=.8 to pick up the lower density clusters
dstream1 <- DSC_DStream(gridsize=1, Cm=1.5)
update(dstream1, stream, 1000)
get_assignment(dstream1,get_points(stream,n=100),type = "micro")
get_weights(dstream1)
get_weights(dstream1,type="micro")
get_weights(dstream1,type="macro")
sum(get_weights(dstream1,type="macro"))
stream <- DSD_BarsAndGaussians(noise=.05)
# we set Cm=.8 to pick up the lower density clusters
dstream1 <- DSC_DStream(gridsize=1, Cm=1.5)
update(dstream1, stream, 1000)
get_assignment(dstream1,get_points(stream,n=100),type = "micro")
get_weights(dstream1,type="micro")
ptm <- proc.time()
update(dstream1, stream, 1000)
proc.time() - ptm
ptm <- Sys.time()
update(dstream1, stream, 1000)
Sys.time() - ptm
ptm <- Sys.time()
update(dstream1, stream, 1000)
Sys.time() - ptm
ptm <- Sys.time()
for (i in 1:1000){
update(dstream1, stream, 1)}
Sys.time() - ptm
ptm <- Sys.time()
update(dstream1, stream, 1000)
Sys.time() - ptm
ptm <- Sys.time()
for (i in 1:1000){
update(dstream1, stream, 1)}
Sys.time() - ptm
ptm <- Sys.time()
update(dstream1, stream, 1000)
Sys.time() - ptm
ptm <- Sys.time()
for (i in 1:1000){
update(dstream1, stream, 1)}
Sys.time() - ptm
0.32/0.015
pnts = get_points(stream,n=1000)
ptm <- Sys.time()
update(dstream1, stream, 1000)
e=get_assignment(dstream1,pnts)
Sys.time() - ptm
ptm <- Sys.time()
for (i in 1:1000){
update(dstream1, stream, 1)
e=get_assignment(dstream1,pnts[i,])
}
Sys.time() - ptm
length(seq(.1,10,.05))
setwd("C:/Users/user/Google Drive/BGU/Research/Context DLP/My Work/Streams/pcStream/GitHub Source Code/bkp/R")
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==0,] #12 is the label for 'normal'
#set pcStream Paramaters
driftThreshold = 4 # Phi: Drift threshold (number of stds away to say that an observation not likeli part of the model)
maxDriftSize = 20 # t_min: Number of instances in a row that have "drifted"
percentVarience = 0.98 # rho
modelMemSize = 500 # m: the number of observatiosn retained by each model
modelLimit = 2000 #|C|:Limits the number of pcStream models to store in the collection. If more than modelLimit clusters are detected, then 'knowledge freshness' merging is performed
print("Starting pcStream...")
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize,modelLimit)
print("done.")
modelCollection = output[[1]]
A = output[[3]] #the transition matrix
print(paste("The number of contexts found:",length(modelCollection)))
##Use the modelCollection to evaluation anomaly detection on the testset:
source('DetectPointAnom.R')
source('DetectContextAnom.R')
source('DetectCollectAnom.R')
library('ROCR') #for measuring performance
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
#collective anomaly detection
windowSize = 5
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==0,] #12 is the label for 'normal'
#set pcStream Paramaters
driftThreshold = 4 # Phi: Drift threshold (number of stds away to say that an observation not likeli part of the model)
maxDriftSize = 20 # t_min: Number of instances in a row that have "drifted"
percentVarience = 0.98 # rho
modelMemSize = 500 # m: the number of observatiosn retained by each model
modelLimit = 2000 #|C|:Limits the number of pcStream models to store in the collection. If more than modelLimit clusters are detected, then 'knowledge freshness' merging is performed
print("Starting pcStream...")
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize,modelLimit)
print("done.")
modelCollection = output[[1]]
A = output[[3]] #the transition matrix
print(paste("The number of contexts found:",length(modelCollection)))
##Use the modelCollection to evaluation anomaly detection on the testset:
source('DetectPointAnom.R')
source('DetectContextAnom.R')
source('DetectCollectAnom.R')
library('ROCR') #for measuring performance
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
#collective anomaly detection
windowSize = 5
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize,modelLimit)
Xtrain
Xtrain = X[1:top,]
Ytest[Ytest==12]
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12]
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
Ytest
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest
Ytest[Ytest==12] = 0
Ytest
Ytest[Ytest!=0] = 1
Ytest
Xtrain
dim(Xtrain)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==0,] #12 is the label for 'normal'
dim(Xtrain)
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
sum(Y[1:top]==0)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==12,] #12 is the label for 'normal'
dim(Ytrain)
dim(Xtrain)
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==12,] #12 is the label for 'normal'
#set pcStream Paramaters
driftThreshold = 4 # Phi: Drift threshold (number of stds away to say that an observation not likeli part of the model)
maxDriftSize = 20 # t_min: Number of instances in a row that have "drifted"
percentVarience = 0.98 # rho
modelMemSize = 500 # m: the number of observatiosn retained by each model
modelLimit = 2000 #|C|:Limits the number of pcStream models to store in the collection. If more than modelLimit clusters are detected, then 'knowledge freshness' merging is performed
print("Starting pcStream...")
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize,modelLimit)
print("done.")
modelCollection = output[[1]]
A = output[[3]] #the transition matrix
print(paste("The number of contexts found:",length(modelCollection)))
##Use the modelCollection to evaluation anomaly detection on the testset:
source('DetectPointAnom.R')
source('DetectContextAnom.R')
source('DetectCollectAnom.R')
library('ROCR') #for measuring performance
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
#collective anomaly detection
windowSize = 5
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
Ytest[Ytest==12] = 0 #12 indicates normal traffic, so it is labeled as benign (0)
Ytest[Ytest!=0] = 1 #All other labels are types of attacks, so they are labled as malicious (1)
#Sanitize the training set (remove all malicous labes so that only the normal behavior is recorded)
Xtrain = Xtrain[Y[1:top,]==12,] #12 is the label for 'normal'
#set pcStream Paramaters
driftThreshold = 4 # Phi: Drift threshold (number of stds away to say that an observation not likeli part of the model)
maxDriftSize = 20 # t_min: Number of instances in a row that have "drifted"
percentVarience = 0.98 # rho
modelMemSize = 500 # m: the number of observatiosn retained by each model
modelLimit = 2000 #|C|:Limits the number of pcStream models to store in the collection. If more than modelLimit clusters are detected, then 'knowledge freshness' merging is performed
print("Starting pcStream...")
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize,modelLimit)
print("done.")
modelCollection = output[[1]]
A = output[[3]] #the transition matrix
print(paste("The number of contexts found:",length(modelCollection)))
##Use the modelCollection to evaluation anomaly detection on the testset:
source('DetectPointAnom.R')
source('DetectContextAnom.R')
source('DetectCollectAnom.R')
library('ROCR') #for measuring performance
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
#collective anomaly detection
windowSize = 5
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
source('DetectContextAnom.R')
#point anomaly detection
scores1 = DetectPointAnom( Xtest, modelCollection, driftThreshold)
AUC1 = performance(prediction(scores1,Ytest), 'auc')@y.values[[1]]
AUC1 = max(AUC1,1-AUC1) #encase the scores are inverted
#contextual anomlay detection
scores2 = DetectContextAnom( Xtest, modelCollection, driftThreshold,A)
AUC2 = performance(prediction(scores2,Ytest), 'auc')@y.values[[1]]
AUC2 = max(AUC2,1-AUC2) #encase the scores are inverted
#collective anomaly detection
windowSize = 5
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
AUC2
#collective anomaly detection
windowSize = 50
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
#collective anomaly detection
windowSize = 5000
scores3 = DetectCollectAnom( Xtest, modelCollection, driftThreshold,A,windowSize)
AUC3 = performance(prediction(scores3,Ytest), 'auc')@y.values[[1]]
AUC3 = max(AUC1,1-AUC1) #encase the scores are inverted
print(paste("The AUC of detecting point anomalies is:",AUC1))
print(paste("The AUC of detecting contextual anomalies is:",AUC2))
print(paste("The AUC of detecting collective anomalies is:",AUC3))
# If you use the source code or impliment pcStream, please cite the following paper:
# Yisroel Mirsky, Bracha Shapira, Lior Rokach, and Yuval Elovici. "pcStream: A Stream Clustering Algorithm for Dynamically Detecting and Managing Temporal Contexts." In Advances in Knowledge Discovery and Data Mining (PAKDD), pp. 119-133. Springer International Publishing, 2015.
#setwd("...")
source('pcStream.R')
source('predict.R')
library(pracma)
X=as.matrix(read.csv("KDD_data_sample.csv",stringsAsFactors=F))
Y=as.matrix(read.csv("KDD_data_sample_labels.csv",stringsAsFactors=F))
#perform zscore on the stream
means = apply(X,2,mean)
stds = apply(X,2,sd)+matrix(.00000000000001,1,ncol(X)) #avoid divide by zero error
X = (X-repmat(means,nrow(X),1))/repmat(stds,nrow(X),1)
#create datasets
top=floor(dim(X)[1]*.8); #take first %80 of stream as train -the rest as test
Xtrain = X[1:top,]
Xtest = X[-(1:top),]
Ytest = Y[-(1:top),]
#set pcStream Paramaters
driftThreshold = 4 # Phi: Drift threshold (number of stds away to say that an observation not likeli part of the model)
maxDriftSize = 20 # t_min: Number of instances in a row that have "drifted"
percentVarience = 0.98 # rho
modelMemSize = 500 # m: the number of observatiosn retained by each model
modelLimit = 2000 #|C|:Limits the number of pcStream models to store in the collection. If more than modelLimit clusters are detected, then 'knowledge freshness' merging is performed
print("Starting pcStream...")
output<-pcStream(Xtrain, driftThreshold, maxDriftSize,percentVarience, modelMemSize, modelLimit)
print("done.")
modelCollection = output[[1]]
print(paste("The number of contexts found:",length(modelCollection)))
#predict labels (cluster assignments) on test set
out = predict(Xtest, modelCollection )
predictions = out[[1]]
#Print the Ajusted Rand Index (performance)
library(mclust)
print(paste("Adjusted Rand Index:",  adjustedRandIndex(predictions,Ytest) ))
#plot the cluster frequency
A = output[[3]] # Transition count matrix
MC = A/repmat(t(t(apply(A,1,sum))),1,dim(A)[1]) #Markov chain
hist(apply(A,1,sum)) #histogram of transition node degrees
plot(sort(apply(A,1,sum))) #Plot of out degrees
